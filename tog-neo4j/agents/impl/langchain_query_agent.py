import asyncio
from typing import Dict, Any, List

# LangChain Core Imports (LangChain 0.1+ è§„èŒƒ)
from langchain_core.tools import tool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import SystemMessage
from langchain_ollama import ChatOllama  # ä½¿ç”¨ Ollama æ›¿ä»£ OpenAI
from langchain.agents import AgentExecutor, create_tool_calling_agent

# å¼•å…¥ä½ ç°æœ‰çš„å·¥å…·ç±»
from agents.tools.query_hybrid import QueryHybridTool
from agents.tools.query_graphrag import QueryGraphRAGTool
from agents.tools.query_tog import QueryToGTool
from core.config import settings
from utils.logger import logger
from utils.java_backend import get_knowledge_bases


class LangChainQueryAgent:
    """
    åŸºäº LangChain 1.0 å®ç°çš„å›¾è°±æŸ¥è¯¢ Agent
    é›†æˆäº† Hybrid, GraphRAG, ToG ä¸‰ç§å·¥å…·
    """

    def __init__(self, model_name: str = None, temperature: float = 0):
        # 1. åˆå§‹åŒ–åŸå§‹å·¥å…·ç±»
        self._hybrid_tool = QueryHybridTool()
        self._graphrag_tool = QueryGraphRAGTool()
        self._tog_tool = QueryToGTool()

        # 2. åˆå§‹åŒ– LLM (ä½¿ç”¨ Ollama)
        model = model_name or settings.llm_model
        self.llm = ChatOllama(
            model=model,
            temperature=temperature,
            base_url=settings.llm_api_url,
            timeout=120
        )

        # 3. æ„å»º LangChain å·¥å…·é›†
        self.tools = self._build_langchain_tools()

        # 4. æ„å»º Agent
        self.agent_executor = self._build_agent_executor()

    def _build_langchain_tools(self) -> List[Any]:
        """
        å°†ç°æœ‰çš„å·¥å…·ç±»æ–¹æ³•å°è£…ä¸º LangChain Tools
        """

        # --- å°è£… Hybrid æŸ¥è¯¢ (Async) ---
        @tool("hybrid_query")
        async def hybrid_query(grag_id: str, question: str, max_depth: int = 5, max_width: int = 5) -> Dict[str, Any]:
            """
            å½“éœ€è¦ç»¼åˆæŸ¥è¯¢æˆ–è€…é—®é¢˜æ¯”è¾ƒå¤æ‚éœ€è¦å¤šè·³æ¨ç†æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚
            ç»“åˆäº† ToG (Think-on-Graph) å’Œ GraphRAG çš„ä¼˜åŠ¿ã€‚
            """
            # ç›´æ¥è°ƒç”¨åŸå§‹ç±»çš„ async execute
            return await self._hybrid_tool.execute(
                grag_id=grag_id,
                question=question,
                max_depth=max_depth,
                max_width=max_width
            )

        # --- å°è£… GraphRAG æŸ¥è¯¢ (Sync) ---
        @tool("graphrag_query")
        def graphrag_query(grag_id: str, question: str, method: str = "local") -> Dict[str, Any]:
            """
            å½“éœ€è¦è¿›è¡Œå…¨å±€æ‘˜è¦ã€ç¤¾åŒºå‘ç°æˆ–å®è§‚ç†è§£å›¾è°±å†…å®¹æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚
            Method å¯ä»¥æ˜¯ 'local' (å±€éƒ¨) æˆ– 'global' (å…¨å±€)ã€‚
            """
            return self._graphrag_tool.execute(
                grag_id=grag_id,
                question=question,
                method=method
            )

        # --- å°è£… ToG æŸ¥è¯¢ (Sync) ---
        @tool("tog_query")
        def tog_query(grag_id: str, question: str, max_depth: int = 5, max_width: int = 5) -> Dict[str, Any]:
            """
            å½“éœ€è¦æ²¿ç€å›¾è°±è·¯å¾„è¿›è¡Œæ·±åº¦æ¨ç†ã€å¯»æ‰¾å®ä½“é—´ç‰¹å®šå…³ç³»é“¾æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚
            ToG ä»£è¡¨ Think-on-Graphã€‚
            """
            return self._tog_tool.execute(
                grag_id=grag_id,
                question=question,
                max_depth=max_depth,
                max_width=max_width
            )

        return [hybrid_query, graphrag_query, tog_query]

    def _build_agent_executor(self) -> AgentExecutor:
        """æ„å»º Agent æ‰§è¡Œå™¨"""

        # å®šä¹‰ Prompt
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†å›¾è°±æŸ¥è¯¢åŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œæä¾›çš„ grag_idï¼ˆå›¾è°±IDï¼‰ï¼Œ"
                       "é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ï¼ˆHybrid, GraphRAG, æˆ– ToGï¼‰æ¥å›ç­”é—®é¢˜ã€‚\n"
                       "1. å¦‚æœé—®é¢˜éœ€è¦å®è§‚ç†è§£æˆ–æ€»ç»“ï¼Œä¼˜å…ˆç”¨ GraphRAGã€‚\n"
                       "2. å¦‚æœé—®é¢˜éœ€è¦å¤šæ­¥æ¨ç†ï¼Œä¼˜å…ˆç”¨ ToGã€‚\n"
                       "3. å¦‚æœä¸ç¡®å®šæˆ–éœ€è¦ç»¼åˆèƒ½åŠ›ï¼Œä½¿ç”¨ Hybridã€‚\n"
                       "è¯·ç¡®ä¿åœ¨è°ƒç”¨å·¥å…·æ—¶ä¼ å…¥æ­£ç¡®çš„ grag_idã€‚"),
            ("user", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])

        # ç»‘å®šå·¥å…·åˆ° LLM
        llm_with_tools = self.llm.bind_tools(self.tools)

        # åˆ›å»º Agent (ä½¿ç”¨ Tool Calling æ¨¡å¼ï¼Œç°ä»£ LLM çš„æ ‡å‡†åšæ³•)
        agent = create_tool_calling_agent(llm_with_tools, self.tools, prompt)

        # åˆ›å»ºæ‰§è¡Œå™¨
        return AgentExecutor(
            agent=agent,
            tools=self.tools,
            verbose=True,
            handle_parsing_errors=True
        )

    async def query(self, grag_id: str, question: str) -> Dict[str, Any]:
        """
        å¯¹å¤–æš´éœ²çš„ç»Ÿä¸€æŸ¥è¯¢æ¥å£
        """
        try:
            # å¦‚æœ grag_id ä¸ºç©ºæˆ–ä¸º defaultï¼Œåˆ™è·å–æ‰€æœ‰çŸ¥è¯†åº“å¹¶å¾ªç¯æŸ¥è¯¢
            if not grag_id or grag_id == "default":
                logger.info("ğŸ“š grag_id ä¸ºç©ºï¼Œå¼€å§‹è·å–çŸ¥è¯†åº“åˆ—è¡¨å¹¶å¾ªç¯æŸ¥è¯¢")
                
                # è·å–çŸ¥è¯†åº“åˆ—è¡¨
                knowledge_bases = await get_knowledge_bases()
                
                if not knowledge_bases:
                    logger.warning("âŒ æœªæ‰¾åˆ°ä»»ä½•çŸ¥è¯†åº“")
                    return {
                        "success": False,
                        "error": "æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„çŸ¥è¯†åº“"
                    }
                
                logger.info(f"âœ… è·å–åˆ° {len(knowledge_bases)} ä¸ªçŸ¥è¯†åº“: {[kb['name'] for kb in knowledge_bases]}")
                
                # éå†çŸ¥è¯†åº“è¿›è¡ŒæŸ¥è¯¢
                for kb in knowledge_bases:
                    kb_grag_id = kb.get("graph_key") or kb.get("grag_id")
                    kb_name = kb.get("name", "æœªçŸ¥çŸ¥è¯†åº“")
                    
                    logger.info(f"[{kb_grag_id}] ğŸ” å°è¯•æŸ¥è¯¢çŸ¥è¯†åº“: {kb_name}")
                    
                    # æ„é€ è¾“å…¥ï¼Œæ˜¾å¼åŒ…å« grag_id ä»¥ä¾¿ Agent å¯ä»¥åœ¨ä¸Šä¸‹æ–‡ä¸­ç†è§£
                    input_text = f"Current Graph ID: {kb_grag_id}\nQuestion: {question}"

                    try:
                        # ä½¿ç”¨ ainvoke å› ä¸ºå…¶ä¸­åŒ…å«å¼‚æ­¥å·¥å…· (Hybrid)
                        response = await self.agent_executor.ainvoke({
                            "input": input_text
                        })
                        
                        result = response["output"]
                        # æ£€æŸ¥ç»“æœæ˜¯å¦æœ‰æ•ˆï¼ˆä¾‹å¦‚ï¼Œç­”æ¡ˆé•¿åº¦å¤§äºä¸€å®šé˜ˆå€¼ï¼‰
                        if result and len(result.strip()) > 20:  # ç®€å•çš„æœ‰æ•ˆæ€§æ£€æŸ¥
                            logger.info(f"[{kb_grag_id}] âœ… åœ¨çŸ¥è¯†åº“ '{kb_name}' ä¸­æ‰¾åˆ°æœ‰æ•ˆç­”æ¡ˆ")
                            
                            return {
                                "success": True,
                                "result": result,
                                "details": response,  # åŒ…å«ä¸­é—´æ­¥éª¤
                                "kb_used": kb_name,
                                "kb_grag_id": kb_grag_id
                            }
                        else:
                            logger.info(f"[{kb_grag_id}] âš ï¸ çŸ¥è¯†åº“ '{kb_name}' æŸ¥è¯¢ç»“æœæ— æ•ˆï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ª")
                            continue
                    except Exception as kb_error:
                        logger.error(f"[{kb_grag_id}] âŒ æŸ¥è¯¢çŸ¥è¯†åº“ '{kb_name}' æ—¶å‡ºé”™: {kb_error}")
                        continue
                
                # æ‰€æœ‰çŸ¥è¯†åº“éƒ½æŸ¥è¯¢å¤±è´¥
                logger.error("âŒ æ‰€æœ‰çŸ¥è¯†åº“æŸ¥è¯¢éƒ½å¤±è´¥")
                return {
                    "success": False,
                    "error": "åœ¨æ‰€æœ‰çŸ¥è¯†åº“ä¸­éƒ½æœªèƒ½æ‰¾åˆ°æ»¡æ„ç­”æ¡ˆ"
                }
            else:
                # åŸæœ‰é€»è¾‘ï¼šæŒ‡å®šçŸ¥è¯†åº“æŸ¥è¯¢
                logger.info(f"Agent æ”¶åˆ°æŸ¥è¯¢: [{grag_id}] {question}")

                # æ„é€ è¾“å…¥ï¼Œæ˜¾å¼åŒ…å« grag_id ä»¥ä¾¿ Agent å¯ä»¥åœ¨ä¸Šä¸‹æ–‡ä¸­ç†è§£
                input_text = f"Current Graph ID: {grag_id}\nQuestion: {question}"

                # ä½¿ç”¨ ainvoke å› ä¸ºå…¶ä¸­åŒ…å«å¼‚æ­¥å·¥å…· (Hybrid)
                response = await self.agent_executor.ainvoke({
                    "input": input_text
                })

                return {
                    "success": True,
                    "result": response["output"],
                    "details": response  # åŒ…å«ä¸­é—´æ­¥éª¤
                }
        except Exception as e:
            logger.error(f"Agent æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e)
            }


# --- ä½¿ç”¨ç¤ºä¾‹ (å¯ä»¥åœ¨ main.py ä¸­è°ƒç”¨) ---
if __name__ == "__main__":
    # æ¨¡æ‹Ÿè¿è¡Œ
    async def main():
        agent = LangChainQueryAgent()

        # ç¤ºä¾‹ï¼šAgent åº”è¯¥ä¼šé€‰æ‹© GraphRAG æˆ– Hybrid
        result = await agent.query(
            grag_id="test_graph_001",
            question="æ€»ç»“ä¸€ä¸‹è¿™ä¸ªçŸ¥è¯†å›¾è°±é‡Œå…³äºäººå·¥æ™ºèƒ½çš„ä¸»è¦è§‚ç‚¹"
        )
        print("Final Answer:", result.get("result"))


    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main())