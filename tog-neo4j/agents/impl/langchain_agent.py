"""
åŸºäº LangChain çš„æ™ºèƒ½æŸ¥è¯¢ Agent - æ·»åŠ  Markdown æ ¼å¼åŒ–æ”¯æŒ
"""
import time
from typing import Dict, Any, List
from langchain_ollama import ChatOllama
from langchain_classic.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from agents.base import BaseAgent, AgentContext, AgentResult
from agents.tools.langchain_tools import get_all_tools, GraphRAGTool, ToGTool, HybridQueryTool
from core.config import settings
from utils.logger import logger
from utils.java_backend import get_knowledge_bases


class LangChainQueryAgent(BaseAgent):
    """åŸºäº LangChain çš„æ™ºèƒ½æŸ¥è¯¢ Agent"""

    def __init__(self):
        super().__init__(
            name="LangChainQueryAgent",
            description="ä½¿ç”¨ LangChain æ¡†æ¶çš„æ™ºèƒ½æŸ¥è¯¢ Agent,æ”¯æŒè‡ªåŠ¨å·¥å…·é€‰æ‹©å’Œæ¨ç†"
        )

        # åˆå§‹åŒ– Ollama LLM
        self.llm = ChatOllama(
            model=settings.llm_model,
            temperature=0.7,
            base_url=settings.llm_api_url,
            timeout=120
        )

        # åˆå§‹åŒ–è§„åˆ’ç”¨çš„ Ollama LLM (ä½æ¸©åº¦)
        self.planning_llm = ChatOllama(
            model=settings.llm_model,
            temperature=0.1,
            base_url=settings.llm_api_url,
            timeout=120
        )

        # è·å–å·¥å…·
        self.tools = get_all_tools()

        # åˆå§‹åŒ–å·¥å…·å®ä¾‹
        self.graphrag_tool = GraphRAGTool()
        self.tog_tool = ToGTool()
        self.hybrid_tool = HybridQueryTool()

        # åˆ›å»º Agent Prompt
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", self._get_system_prompt()),
            MessagesPlaceholder(variable_name="chat_history", optional=True),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])

        # åˆ›å»º Agent
        self.agent = create_tool_calling_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=self.prompt
        )

        # åˆ›å»º Agent Executor
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            verbose=True,
            max_iterations=20,
            max_execution_time=300,
            handle_parsing_errors=True,
            return_intermediate_steps=True
        )

        logger.info("âœ… LangChainQueryAgent åˆå§‹åŒ–å®Œæˆ")

    def _get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤º"""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†å›¾è°±æŸ¥è¯¢åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·é—®é¢˜,é€‰æ‹©æœ€åˆé€‚çš„æŸ¥è¯¢å·¥å…·æ¥è·å–ç­”æ¡ˆã€‚

        å¯ç”¨å·¥å…·è¯´æ˜:
        1. **graphrag_query**: é€‚ç”¨äºç®€å•çš„äº‹å®æŸ¥è¯¢å’Œä¿¡æ¯æ£€ç´¢,æ£€ç´¢é€Ÿåº¦è¾ƒå¿«
        2. **tog_query**: é€‚ç”¨äºéœ€è¦å¤šæ­¥æ¨ç†å’Œé€»è¾‘é“¾çš„å¤æ‚é—®é¢˜,æ£€ç´¢é€Ÿåº¦è¾ƒæ…¢
        3. **hybrid_query**: é€‚ç”¨äºéœ€è¦æ·±åº¦æ¨ç†å’Œå¹¿æ³›æ£€ç´¢çš„å¤æ‚é—®é¢˜,æ£€ç´¢é€Ÿåº¦æœ€æ…¢

        å·¥å…·é€‰æ‹©ç­–ç•¥:
        - ç®€å•é—®é¢˜ (å¦‚"ä»€ä¹ˆæ˜¯X"ã€"Xçš„å®šä¹‰") â†’ ä½¿ç”¨ graphrag_query
        - ä¸­ç­‰å¤æ‚é—®é¢˜ (å¦‚"Xå’ŒYçš„å…³ç³»"ã€"Xå¦‚ä½•å½±å“Y") â†’ ä½¿ç”¨ tog_query
        - å¤æ‚é—®é¢˜ (å¦‚"åˆ†æXçš„å¤šæ–¹é¢å½±å“"ã€"æ¯”è¾ƒXå’ŒYçš„ä¼˜ç¼ºç‚¹") â†’ ä½¿ç”¨ hybrid_query

        **ç­”æ¡ˆæ ¼å¼è¦æ±‚** (é‡è¦):
        1. ä½¿ç”¨ Markdown æ ¼å¼è¾“å‡ºç­”æ¡ˆ
        2. ä½¿ç”¨æ¸…æ™°çš„æ®µè½åˆ†éš”,æ¯ä¸ªæ®µè½ç”¨ç©ºè¡Œåˆ†å¼€
        3. ä½¿ç”¨åˆ—è¡¨ã€åŠ ç²—ç­‰ Markdown å…ƒç´ æé«˜å¯è¯»æ€§
        4. å¤šç‚¹å†…å®¹ä½¿ç”¨ç¼–å·åˆ—è¡¨ (1. 2. 3. ...) æˆ–æ— åºåˆ—è¡¨ (-)
        5. å…³é”®ä¿¡æ¯ä½¿ç”¨åŠ ç²—å¼ºè°ƒ
        6. ä¿æŒç®€æ´ã€ç»“æ„åŒ–çš„å›ç­”é£æ ¼

        **ç­”æ¡ˆç»“æ„ç¤ºä¾‹**:
        ```markdown
        æ ¹æ®æŸ¥è¯¢ç»“æœ,è¿™é‡Œæ˜¯ç­”æ¡ˆ:

        1. **ç¬¬ä¸€ç‚¹**: è¯¦ç»†è¯´æ˜
        2. **ç¬¬äºŒç‚¹**: è¯¦ç»†è¯´æ˜
        3. **ç¬¬ä¸‰ç‚¹**: è¯¦ç»†è¯´æ˜

        **æ€»ç»“**: æ•´ä½“æ€»ç»“
        ```

        é‡è¦æç¤º:
        1. é¦–å…ˆåˆ†æé—®é¢˜å¤æ‚åº¦
        2. é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·
        3. å¦‚æœç¬¬ä¸€æ¬¡æŸ¥è¯¢å¤±è´¥æˆ–ç»“æœä¸æ»¡æ„,å¯ä»¥å°è¯•å…¶ä»–å·¥å…·
        4. ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿° Markdown æ ¼å¼è¦æ±‚è¾“å‡ºç­”æ¡ˆ
        5. å¦‚æœæŸ¥è¯¢å¤±è´¥,è¦æä¾›æœ‰ç”¨çš„é”™è¯¯ä¿¡æ¯

        è¯·å§‹ç»ˆä»¥ç”¨æˆ·çš„é—®é¢˜ä¸ºä¸­å¿ƒ,æä¾›å‡†ç¡®ã€æœ‰ç”¨ã€æ ¼å¼è‰¯å¥½çš„ç­”æ¡ˆã€‚"""

    def can_handle(self, context: AgentContext) -> bool:
        """åˆ¤æ–­æ˜¯å¦èƒ½å¤„ç†è¯¥ä»»åŠ¡"""
        return True

    async def execute(self, context: AgentContext) -> AgentResult:
        """æ‰§è¡ŒæŸ¥è¯¢"""
        start_time = time.time()

        try:
            logger.info(f"[{context.grag_id}] ğŸ¤– LangChainQueryAgent å¼€å§‹æ‰§è¡Œ")
            logger.info(f"[{context.grag_id}] ğŸ’¬ é—®é¢˜: {context.question}")

            # å¦‚æœ grag_id ä¸ºç©ºæˆ–ä¸º default,åˆ™è·å–æ‰€æœ‰çŸ¥è¯†åº“å¹¶å¾ªç¯æŸ¥è¯¢
            if not context.grag_id or context.grag_id == "default":
                logger.info("ğŸ“š grag_id ä¸ºç©º,å¼€å§‹è·å–çŸ¥è¯†åº“åˆ—è¡¨å¹¶å¾ªç¯æŸ¥è¯¢")

                knowledge_bases = await get_knowledge_bases()

                if not knowledge_bases:
                    logger.warning("âŒ æœªæ‰¾åˆ°ä»»ä½•çŸ¥è¯†åº“")
                    return AgentResult(
                        success=False,
                        data=None,
                        message="æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„çŸ¥è¯†åº“",
                        error="no_knowledge_bases_found",
                        execution_time=time.time() - start_time
                    )

                logger.info(f"âœ… è·å–åˆ° {len(knowledge_bases)} ä¸ªçŸ¥è¯†åº“: {[kb['name'] for kb in knowledge_bases]}")

                # éå†çŸ¥è¯†åº“è¿›è¡ŒæŸ¥è¯¢
                for kb in knowledge_bases:
                    kb_grag_id = kb.get("graph_key") or kb.get("grag_id")
                    kb_name = kb.get("name", "æœªçŸ¥çŸ¥è¯†åº“")

                    if not kb_grag_id:
                        logger.warning(f"âš ï¸ çŸ¥è¯†åº“ '{kb_name}' ç¼ºå°‘ graph_key æˆ– grag_id,è·³è¿‡")
                        continue

                    logger.info(f"[{kb_grag_id}] ğŸ” å°è¯•æŸ¥è¯¢çŸ¥è¯†åº“: {kb_name}")

                    kb_context = AgentContext(
                        grag_id=kb_grag_id,
                        question=context.question,
                        conversation_history=context.conversation_history,
                        metadata=context.metadata
                    )

                    result = await self._execute_single_query(kb_context, kb_name, validate_answer=False)

                    if result.data and result.data.get("answer"):
                        answer = result.data["answer"]
                        is_valid = await self._validate_answer(context.question, answer)

                        if is_valid:
                            logger.info(f"[{kb_grag_id}] âœ… åœ¨çŸ¥è¯†åº“ '{kb_name}' ä¸­æ‰¾åˆ°æœ‰æ•ˆç­”æ¡ˆ")

                            execution_time = time.time() - start_time
                            result.execution_time = execution_time
                            result.success = True
                            result.message = f"åœ¨çŸ¥è¯†åº“ '{kb_name}' ä¸­æŸ¥è¯¢æˆåŠŸ"
                            return result
                        else:
                            logger.info(f"[{kb_grag_id}] âš ï¸ çŸ¥è¯†åº“ '{kb_name}' ç­”æ¡ˆéªŒè¯å¤±è´¥,ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ª")
                            continue
                    else:
                        logger.info(f"[{kb_grag_id}] âš ï¸ çŸ¥è¯†åº“ '{kb_name}' æŸ¥è¯¢å¤±è´¥,ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ª")
                        continue

                logger.error("âŒ æ‰€æœ‰çŸ¥è¯†åº“æŸ¥è¯¢éƒ½å¤±è´¥")
                execution_time = time.time() - start_time
                return AgentResult(
                    success=False,
                    data=None,
                    message="åœ¨æ‰€æœ‰çŸ¥è¯†åº“ä¸­éƒ½æœªèƒ½æ‰¾åˆ°æ»¡æ„ç­”æ¡ˆ",
                    error="all_knowledge_bases_failed",
                    execution_time=execution_time
                )
            else:
                logger.info(f"[{context.grag_id}] ğŸ¯ æ‰§è¡ŒæŒ‡å®šçŸ¥è¯†åº“æŸ¥è¯¢")

                result = await self._execute_single_query(context, "æŒ‡å®šçŸ¥è¯†åº“")
                execution_time = time.time() - start_time
                result.execution_time = execution_time
                return result

        except Exception as e:
            logger.error(f"LangChainQueryAgent æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return AgentResult(
                success=False,
                data=None,
                message="æŸ¥è¯¢å¤±è´¥",
                error=str(e),
                execution_time=time.time() - start_time
            )

    def _prepare_input(self, context: AgentContext) -> Dict[str, Any]:
        """å‡†å¤‡ Agent è¾“å…¥"""
        chat_history = []
        for msg in context.conversation_history:
            if msg["role"] == "user":
                chat_history.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                chat_history.append(AIMessage(content=msg["content"]))

        input_text = f"""
çŸ¥è¯†åº“ID: {context.grag_id}
ç”¨æˆ·é—®é¢˜: {context.question}

è¯·é€‰æ‹©åˆé€‚çš„å·¥å…·æŸ¥è¯¢å¹¶è¿”å›ç­”æ¡ˆã€‚
"""

        return {
            "input": input_text,
            "chat_history": chat_history
        }

    def _extract_tools_used(self, result: Dict[str, Any]) -> List[str]:
        """ä»ç»“æœä¸­æå–ä½¿ç”¨çš„å·¥å…·"""
        intermediate_steps = result.get("intermediate_steps", [])
        tools_used = []
        for step in intermediate_steps:
            if len(step) > 0:
                action = step[0]
                tools_used.append(action.tool)
        return tools_used

    def _parse_result(
        self,
        result: Dict[str, Any],
        context: AgentContext,
        execution_time: float
    ) -> AgentResult:
        """è§£æ Agent æ‰§è¡Œç»“æœ"""
        output = result.get("output", "")
        intermediate_steps = result.get("intermediate_steps", [])

        tools_used = self._extract_tools_used(result)

        logger.info(f"âœ… æŸ¥è¯¢å®Œæˆ,ä½¿ç”¨çš„å·¥å…·: {', '.join(tools_used)}")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {execution_time:.2f}ç§’")

        return AgentResult(
            success=True,
            data={
                "question": context.question,
                "answer": output,
                "grag_id": context.grag_id,
                "tools_used": tools_used
            },
            message="æŸ¥è¯¢æˆåŠŸ",
            execution_time=execution_time,
            metadata={
                "tools_used": tools_used,
                "intermediate_steps": len(intermediate_steps)
            }
        )

    async def _execute_single_query(
        self,
        context: AgentContext,
        kb_name: str = "æœªçŸ¥çŸ¥è¯†åº“",
        validate_answer: bool = True
    ) -> AgentResult:
        """æ‰§è¡Œå•ä¸ªçŸ¥è¯†åº“çš„å®Œæ•´æŸ¥è¯¢æµç¨‹

        Args:
            context: æŸ¥è¯¢ä¸Šä¸‹æ–‡
            kb_name: çŸ¥è¯†åº“åç§°
            validate_answer: æ˜¯å¦éªŒè¯ç­”æ¡ˆè´¨é‡

        Returns:
            AgentResult: æŸ¥è¯¢ç»“æœ
        """
        start_time = time.time()

        # åˆ†æé—®é¢˜å¤æ‚åº¦
        complexity = await self._analyze_complexity(context.question)

        # é€‰æ‹©æŸ¥è¯¢æ–¹æ³•
        method = await self._select_method(complexity, context.question)

        # æ ¹æ®é€‰æ‹©çš„æ–¹æ³•æ‰§è¡ŒæŸ¥è¯¢
        raw_result = await self._execute_query_with_method(context.grag_id, context.question, method)

        # ğŸ”¥ æ–°å¢: æ ¼å¼åŒ–ä¸º Markdown
        # formatted_result = await self._format_to_markdown(context.question, raw_result)
        formatted_result = raw_result

        tools_used = [method]

        execution_time = time.time() - start_time

        is_valid = True
        if validate_answer:
            is_valid = await self._validate_answer(context.question, formatted_result)

        return AgentResult(
            success=is_valid,
            data={
                "question": context.question,
                "answer": formatted_result,  # è¿”å›æ ¼å¼åŒ–åçš„ç»“æœ
                "grag_id": context.grag_id,
                "execution_time": execution_time,
                "kb_name": kb_name
            },
            message="æŸ¥è¯¢æˆåŠŸ" if is_valid else "æŸ¥è¯¢å¤±è´¥",
            execution_time=execution_time,
            metadata={
                "complexity": complexity,
                "method": method,
                "tools_used": tools_used
            }
        )

    async def _format_to_markdown(self, question: str, raw_answer: str) -> str:
        """å°†åŸå§‹ç­”æ¡ˆæ ¼å¼åŒ–ä¸º Markdown æ ¼å¼

        Args:
            question: åŸå§‹é—®é¢˜
            raw_answer: åŸå§‹ç­”æ¡ˆ

        Returns:
            str: Markdown æ ¼å¼çš„ç­”æ¡ˆ
        """
        logger.info("ğŸ“ æ ¼å¼åŒ–ç­”æ¡ˆä¸º Markdown...")

        try:
            from langchain_core.messages import HumanMessage

            format_prompt = f"""è¯·å°†ä»¥ä¸‹ç­”æ¡ˆé‡æ–°æ ¼å¼åŒ–ä¸ºæ¸…æ™°çš„ Markdown æ ¼å¼ã€‚

åŸå§‹é—®é¢˜: {question}

åŸå§‹ç­”æ¡ˆ:
{raw_answer}

æ ¼å¼åŒ–è¦æ±‚:
1. ä½¿ç”¨æ¸…æ™°çš„æ®µè½åˆ†éš”,æ¯ä¸ªæ®µè½ç”¨ç©ºè¡Œåˆ†å¼€
2. ä½¿ç”¨ **ç²—ä½“** å¼ºè°ƒå…³é”®ä¿¡æ¯
3. ä½¿ç”¨ç¼–å·åˆ—è¡¨ (1. 2. 3.) æˆ–æ— åºåˆ—è¡¨ (-) ç»„ç»‡å¤šç‚¹å†…å®¹
4. å¦‚æœæœ‰æ€»ç»“,ä½¿ç”¨ **æ€»ç»“:** æ ‡è®°
5. ä¿æŒå†…å®¹å®Œæ•´,ä¸è¦åˆ å‡ä¿¡æ¯
6. ç¡®ä¿é€»è¾‘æ¸…æ™°,ç»“æ„åˆ†æ˜

è¯·ç›´æ¥è¾“å‡ºæ ¼å¼åŒ–åçš„ Markdown æ–‡æœ¬,ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–è¯´æ˜ã€‚"""

            # ä½¿ç”¨ invoke æ–¹æ³•
            response = self.llm.invoke([HumanMessage(content=format_prompt)])

            formatted_answer = response.content.strip()
            logger.info("âœ… Markdown æ ¼å¼åŒ–å®Œæˆ")
            return formatted_answer

        except Exception as e:
            logger.error(f"Markdown æ ¼å¼åŒ–å¤±è´¥: {e}")
            # å¦‚æœæ ¼å¼åŒ–å¤±è´¥,è¿”å›åŸå§‹ç­”æ¡ˆ
            return raw_answer

    async def _analyze_complexity(self, question: str) -> str:
        """åˆ†æé—®é¢˜å¤æ‚åº¦"""
        logger.info("ğŸ“Š åˆ†æé—®é¢˜å¤æ‚åº¦...")

        try:
            prompt = f"åˆ†æä»¥ä¸‹é—®é¢˜çš„å¤æ‚åº¦,è¿”å›: simple, moderate, æˆ– complex\né—®é¢˜: {question}"

            # ä½¿ç”¨åŒæ­¥è°ƒç”¨
            response = self.planning_llm.invoke(prompt)

            complexity = "moderate"
            content = response.content.lower()
            if "simple" in content:
                complexity = "simple"
            elif "complex" in content:
                complexity = "complex"

            logger.info(f"âœ… å¤æ‚åº¦: {complexity}")
            return complexity

        except Exception as e:
            logger.error(f"åˆ†æå¤æ‚åº¦å¤±è´¥: {e}")
            return "moderate"  # é»˜è®¤è¿”å›ä¸­ç­‰å¤æ‚åº¦

    async def _select_method(self, complexity: str, question: str) -> str:
        """é€‰æ‹©æŸ¥è¯¢æ–¹æ³•"""
        logger.info("ğŸ¯ é€‰æ‹©æŸ¥è¯¢æ–¹æ³•...")

        question_lower = question.lower()

        if "tog" in question_lower or "æ€ç»´å›¾" in question_lower:
            method = "tog"
        elif "graphrag" in question_lower:
            method = "graphrag"
        elif "æ··åˆ" in question_lower or "hybrid" in question_lower:
            method = "hybrid"
        else:
            if complexity == "simple":
                method = "graphrag"
            elif complexity == "complex":
                method = "hybrid"
            else:
                method = "tog"

        logger.info(f"âœ… é€‰æ‹©æ–¹æ³•: {method}")

        return method

    async def _execute_query_with_method(self, grag_id: str, question: str, method: str) -> str:
        """æ ¹æ®é€‰æ‹©çš„æ–¹æ³•æ‰§è¡ŒæŸ¥è¯¢"""
        logger.info(f"ğŸ” ä½¿ç”¨æ–¹æ³• {method} æŸ¥è¯¢çŸ¥è¯†åº“: {grag_id}")

        try:
            if method == "graphrag":
                result = self.graphrag_tool._run(grag_id, question)
            elif method == "tog":
                result = self.tog_tool._run(grag_id, question)
            else:  # hybrid
                result = await self.hybrid_tool._arun(grag_id, question)

            logger.info(f"âœ… {method} æŸ¥è¯¢å®Œæˆ")
            return result

        except Exception as e:
            error_msg = f"æŸ¥è¯¢å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return error_msg

    async def _validate_answer(self, question: str, answer: str) -> bool:
        """ä½¿ç”¨å¤§æ¨¡å‹éªŒè¯ç­”æ¡ˆè´¨é‡"""
        logger.info("ğŸ¤– ä½¿ç”¨å¤§æ¨¡å‹éªŒè¯ç­”æ¡ˆè´¨é‡...")

        if not answer or not isinstance(answer, str):
            logger.info("âŒ ç­”æ¡ˆä¸ºç©ºæˆ–éå­—ç¬¦ä¸²")
            return False

        cleaned_answer = answer.strip()

        if len(cleaned_answer) < 20:
            logger.info(f"âŒ ç­”æ¡ˆé•¿åº¦ä¸è¶³ ({len(cleaned_answer)} å­—ç¬¦)")
            return False

        try:
            from langchain_core.messages import HumanMessage

            validation_prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹ç­”æ¡ˆæ˜¯å¦æœ‰æ•ˆå›ç­”äº†ç”¨æˆ·çš„é—®é¢˜ã€‚
                
ç”¨æˆ·é—®é¢˜: {question}

AIå›ç­”: {answer}

è¯·ä»…å›ç­” "VALID" å¦‚æœå›ç­”æœ‰æ•ˆ,å¦åˆ™å›ç­” "INVALID"ã€‚åˆ¤æ–­æ ‡å‡†:
1. å›ç­”æ˜¯å¦ç›´æ¥é’ˆå¯¹é—®é¢˜
2. å›ç­”æ˜¯å¦æä¾›äº†æœ‰ç”¨ä¿¡æ¯
3. å›ç­”æ˜¯å¦å®Œæ•´æˆ–è‡³å°‘æä¾›äº†éƒ¨åˆ†æœ‰ç”¨ä¿¡æ¯
4. å›ç­”æ˜¯å¦ä¸æ˜¯æ‹’ç»å›ç­”æˆ–é”™è¯¯ä¿¡æ¯

è¯„ä¼°:"""

            # ä½¿ç”¨ invoke æ–¹æ³•
            response = self.planning_llm.invoke([HumanMessage(content=validation_prompt)])

            content = response.content.strip().upper()
            is_valid = "VALID" in content and "INVALID" not in content

            logger.info(f"âœ… å¤§æ¨¡å‹è¯„ä¼°ç»“æœ: {'æœ‰æ•ˆ' if is_valid else 'æ— æ•ˆ'}")
            return is_valid

        except Exception as e:
            logger.error(f"å¤§æ¨¡å‹éªŒè¯ç­”æ¡ˆæ—¶å‡ºé”™: {e}")
            return self._fallback_validate_answer(answer)

    def _fallback_validate_answer(self, answer: str) -> bool:
        """å¤‡ç”¨ç­”æ¡ˆéªŒè¯æ–¹æ³•"""
        logger.info("ğŸ”„ ä½¿ç”¨å¤‡ç”¨éªŒè¯æ–¹æ³•")

        if not answer or not isinstance(answer, str):
            logger.info("âŒ ç­”æ¡ˆä¸ºç©ºæˆ–éå­—ç¬¦ä¸²")
            return False

        cleaned_answer = answer.strip()

        if len(cleaned_answer) < 20:
            logger.info(f"âŒ ç­”æ¡ˆé•¿åº¦ä¸è¶³ ({len(cleaned_answer)} å­—ç¬¦)")
            return False

        logger.info("âœ… é€šè¿‡å¤‡ç”¨éªŒè¯")
        return True