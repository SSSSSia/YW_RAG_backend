"""
æ··åˆæŸ¥è¯¢ä¸šåŠ¡é€»è¾‘
"""
import time
from pathlib import Path
from utils.logger import logger,log_step
from services.query_tog_service import ToGService
from services.query_graphrag_service import GraphRAGService
from core.llm_client import llm_client


class HybridQueryService:
    """ToG+GraphRAGæ··åˆæŸ¥è¯¢æœåŠ¡"""

    def __init__(self, grag_id: str, max_depth: int = 5, max_width: int = 5, method: str = "local"):
        self.grag_id = grag_id
        self.max_depth = max_depth
        self.max_width = max_width
        self.method = method

    async def query(self, question: str) -> dict:
        """æ‰§è¡Œæ··åˆæŸ¥è¯¢"""
        start_time = time.time()

        logger.info(f"[{self.grag_id}] ðŸ” å¼€å§‹ToG+GraphRAGæ··åˆæŸ¥è¯¢")

        # æ‰§è¡ŒToGæŸ¥è¯¢
        log_step(1, 4, "æ‰§è¡ŒToGæŸ¥è¯¢", self.grag_id)
        tog_service = ToGService(self.grag_id, self.max_depth, self.max_width)
        tog_result = tog_service.reason(question)
        tog_answer = tog_result.get("answer", "")
        tog_success = tog_result.get("success", False)

        # æ‰§è¡ŒGraphRAGæŸ¥è¯¢
        log_step(2, 4, "æ‰§è¡ŒGraphRAGæŸ¥è¯¢", self.grag_id)
        graphrag_service = GraphRAGService(self.grag_id)
        graph_success, graph_answer, _ = graphrag_service.query(question, self.method)

        # æ•´åˆç­”æ¡ˆ
        log_step(3, 4, "æ•´åˆä¸¤ä¸ªç­”æ¡ˆ", self.grag_id)
        if not tog_answer and not graph_answer:
            return {
                "success": False,
                "question": question,
                "error": "ä¸¤ç§æŸ¥è¯¢æ–¹æ³•éƒ½æœªè¿”å›žæœ‰æ•ˆç­”æ¡ˆ"
            }

        integration_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†å›¾è°±æŸ¥è¯¢åŠ©æ‰‹ã€‚æˆ‘ä½¿ç”¨ä¸¤ç§ä¸åŒçš„æ–¹æ³•æŸ¥è¯¢äº†åŒä¸€ä¸ªé—®é¢˜ï¼ŒçŽ°åœ¨éœ€è¦ä½ æ•´åˆä¸¤ä¸ªç­”æ¡ˆï¼Œç»™å‡ºæœ€å‡†ç¡®ã€æœ€å…¨é¢çš„å›žç­”ã€‚

        **é—®é¢˜ï¼š** {question}

        **æ–¹æ³•1 - ToGï¼ˆæ€ç»´å›¾è°±ï¼‰çš„ç­”æ¡ˆï¼š**
        {tog_answer if tog_answer else "(æœªèŽ·å–åˆ°ç­”æ¡ˆ)"}

        **æ–¹æ³•2 - GraphRAGçš„ç­”æ¡ˆï¼š**
        {graph_answer if graph_answer else "(æœªèŽ·å–åˆ°ç­”æ¡ˆ)"}

        è¯·ç»¼åˆä»¥ä¸Šä¸¤ä¸ªç­”æ¡ˆï¼Œç»™å‡ºä¸€ä¸ªæœ€ç»ˆç­”æ¡ˆã€‚è¦æ±‚ï¼š
        1. **å†…å®¹æ•´åˆ**ï¼šç»¼åˆä¸¤ä¸ªç­”æ¡ˆçš„ä¼˜ç‚¹å’Œè¡¥å……ä¿¡æ¯ï¼ŒåŽ»é‡å¹¶ç¡®ä¿å‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€‚
        2. **æ ¼å¼è¦æ±‚**ï¼šè¯·ä½¿ç”¨ **Markdown** æ ¼å¼è¿›è¡Œç¾ŽåŒ–ï¼Œè§„èŒƒå¦‚ä¸‹ï¼š
           - ä½¿ç”¨ `###` æ ‡è®°ä¸»è¦çš„å°èŠ‚æ ‡é¢˜ã€‚
           - å…³é”®å®žä½“ã€æ ¸å¿ƒç»“è®ºæˆ–é‡è¦æ•°æ®è¯·ä½¿ç”¨ `**ç²—ä½“**` å¼ºè°ƒã€‚
           - ä½¿ç”¨ `1.` æˆ– `-` åˆ—è¡¨å½¢å¼æ¸…æ™°åœ°åˆ†ç‚¹é™ˆè¿°ã€‚
        3. **å†²çªå¤„ç†**ï¼šå¦‚æžœä¸¤ä¸ªç­”æ¡ˆå­˜åœ¨å†²çªæˆ–çŸ›ç›¾ï¼Œè¯·åŠ¡å¿…åœ¨æ–‡æœ«å¢žåŠ ä¸€ä¸ª `### âš ï¸ ç­”æ¡ˆå·®å¼‚ä¸Žè¯´æ˜Ž` çš„ç‹¬ç«‹ç« èŠ‚ï¼Œè¯¦ç»†åˆ—å‡ºå†²çªç‚¹å¹¶è¯´æ˜Žä½ çš„åˆ¤æ–­ä¾æ®ã€‚
        4. **ç»“æž„æ¸…æ™°**ï¼šç¡®ä¿è¾“å‡ºå†…å®¹å±‚çº§åˆ†æ˜Žï¼Œä¾¿äºŽå‰ç«¯ç›´æŽ¥æ¸²æŸ“å±•ç¤ºã€‚

        **æœ€ç»ˆç­”æ¡ˆï¼š**"""

        log_step(4, 4, "ä½¿ç”¨å¤§æ¨¡åž‹ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ", self.grag_id)
        try:
            final_answer = await llm_client.generate(integration_prompt)
        except Exception as e:
            logger.error(f"æ•´åˆç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            final_answer = tog_answer if len(tog_answer) > len(graph_answer) else graph_answer

        execution_time = time.time() - start_time

        return {
            "success": True,
            "question": question,
            "final_answer": final_answer,
            "tog_answer": tog_answer,
            "graphrag_answer": graph_answer,
            "execution_time": execution_time
        }