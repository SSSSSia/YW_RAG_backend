"""
æ··åˆæŸ¥è¯¢ä¸šåŠ¡é€»è¾‘
"""
import time
from pathlib import Path
from utils.logger import logger,log_step
from services.query_tog_service import ToGService
from services.query_graphrag_service import GraphRAGService
from core.llm_client import llm_client


class HybridQueryService:
    """ToG+GraphRAGæ··åˆæŸ¥è¯¢æœåŠ¡"""

    def __init__(self, grag_id: str, max_depth: int = 5, max_width: int = 5, method: str = "local"):
        self.grag_id = grag_id
        self.max_depth = max_depth
        self.max_width = max_width
        self.method = method

    async def query(self, question: str) -> dict:
        """æ‰§è¡Œæ··åˆæŸ¥è¯¢"""
        start_time = time.time()

        logger.info(f"[{self.grag_id}] ðŸ” å¼€å§‹ToG+GraphRAGæ··åˆæŸ¥è¯¢")

        # æ‰§è¡ŒToGæŸ¥è¯¢
        log_step(1, 4, "æ‰§è¡ŒToGæŸ¥è¯¢", self.grag_id)
        tog_service = ToGService(self.grag_id, self.max_depth, self.max_width)
        tog_result = tog_service.reason(question)
        tog_answer = tog_result.get("answer", "")
        tog_success = tog_result.get("success", False)

        # æ‰§è¡ŒGraphRAGæŸ¥è¯¢
        log_step(2, 4, "æ‰§è¡ŒGraphRAGæŸ¥è¯¢", self.grag_id)
        graphrag_service = GraphRAGService(self.grag_id)
        graph_success, graph_answer, _ = graphrag_service.query(question, self.method)

        # æ•´åˆç­”æ¡ˆ
        log_step(3, 4, "æ•´åˆä¸¤ä¸ªç­”æ¡ˆ", self.grag_id)
        if not tog_answer and not graph_answer:
            return {
                "success": False,
                "question": question,
                "error": "ä¸¤ç§æŸ¥è¯¢æ–¹æ³•éƒ½æœªè¿”å›žæœ‰æ•ˆç­”æ¡ˆ"
            }

        integration_prompt = f"""ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±æŸ¥è¯¢åŠ©æ‰‹ã€‚æˆ‘ä½¿ç”¨ä¸¤ç§ä¸åŒçš„æ–¹æ³•æŸ¥è¯¢äº†åŒä¸€ä¸ªé—®é¢˜ï¼ŒçŽ°åœ¨éœ€è¦ä½ æ•´åˆä¸¤ä¸ªç­”æ¡ˆï¼Œç»™å‡ºæœ€å‡†ç¡®ã€æœ€å…¨é¢çš„å›žç­”ã€‚

**é—®é¢˜ï¼š** {question}

**æ–¹æ³•1 - ToGï¼ˆæ€ç»´å›¾è°±ï¼‰çš„ç­”æ¡ˆï¼š**
{tog_answer if tog_answer else "(æœªèŽ·å–åˆ°ç­”æ¡ˆ)"}

**æ–¹æ³•2 - GraphRAGçš„ç­”æ¡ˆï¼š**
{graph_answer if graph_answer else "(æœªèŽ·å–åˆ°ç­”æ¡ˆ)"}

è¯·ç»¼åˆä»¥ä¸Šä¸¤ä¸ªç­”æ¡ˆï¼Œç»™å‡ºä¸€ä¸ªæœ€ç»ˆç­”æ¡ˆã€‚è¦æ±‚ï¼š
1. ç»¼åˆä¸¤ä¸ªç­”æ¡ˆçš„ä¼˜ç‚¹å’Œè¡¥å……ä¿¡æ¯
2. é¿å…é‡å¤
3. ç¡®ä¿å›žç­”çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
4. å¦‚æžœä¸¤ä¸ªç­”æ¡ˆæœ‰å†²çªï¼Œè¯´æ˜Žä½ çš„åˆ¤æ–­ä¾æ®
5. ç”¨æ¸…æ™°ã€ç»“æž„åŒ–çš„æ–¹å¼(1ã€2ã€3...)å‘ˆçŽ°ç­”æ¡ˆ

æœ€ç»ˆç­”æ¡ˆï¼š"""

        log_step(4, 4, "ä½¿ç”¨å¤§æ¨¡åž‹ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ", self.grag_id)
        try:
            final_answer = await llm_client.generate(integration_prompt)
        except Exception as e:
            logger.error(f"æ•´åˆç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            final_answer = tog_answer if len(tog_answer) > len(graph_answer) else graph_answer

        execution_time = time.time() - start_time

        return {
            "success": True,
            "question": question,
            "final_answer": final_answer,
            "tog_answer": tog_answer,
            "graphrag_answer": graph_answer,
            "execution_time": execution_time
        }